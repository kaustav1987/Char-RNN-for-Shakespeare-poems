{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is based off of Andrej Karpathy's post on RNNs and implementation in Torch.The network will train character by character on some text, then generate new text character by character. Here I have trained the model using Shakespeare's poems.\n",
    "\n",
    "<img src=\"shakespeare.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "##text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There are only 38 unique charecters here in my text \n",
    "\n",
    "text = text.lower()\n",
    "len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(tuple(set(text)))\n",
    "words_count = Counter(words)\n",
    "##sorted_words = sorted(words_count, key= words_count.get, reverse = True)\n",
    "\n",
    "## sorting not required here since this is a char RNN\n",
    "\n",
    "int2char = dict(enumerate(words_count))\n",
    "char2int = {ch:ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode the text\n",
    "\n",
    "encoded = []\n",
    "for ch in text:\n",
    "    encoded.append(char2int[ch])\n",
    "encoded = np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting each charecter into one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr,n_labels):\n",
    "    ##rows = np.multiply(arr.shape[0],arr.shape[1])\n",
    "    rows = np.multiply(*arr.shape)\n",
    "    one_hot = np.zeros((rows , n_labels ),dtype = np.int32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot= one_hot.reshape(arr.shape[0],arr.shape[1],n_labels)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = np.array([[1,2,3]])\n",
    "one_hot(test_seq, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We just want batches to fill all the batches. So the total length should be N * M * K\n",
    "###### where N = Batch_size , M = Seq_len K = No of Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_len):\n",
    "    total_batches = len(arr)//(batch_size*seq_len)\n",
    "    total_arr_length = batch_size*seq_len*total_batches  ## N * M * K\n",
    "    \n",
    "    arr =  arr [ : total_arr_length]\n",
    "    \n",
    "    ## We will use Batch First =True going forward\n",
    "    ## This would mean the no of columns is equal to M*K\n",
    "    ## So in one loop we will just yield M columns...and this would happen K times\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "    \n",
    "    for n in (0, arr.shape[1], seq_len):\n",
    "        x = arr[:,n:n+seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        print(y.shape)\n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = arr[:,n+seq_len]\n",
    "        except IndexError: ## for the last Seq_length\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = arr[:,0]\n",
    "            \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 19 16  1 30 26 25 25 16 31]\n",
      " [15 20 25 18  1 24 16 25  8  1]\n",
      " [19 16 30 16  1 14 32 29 20 26]\n",
      " [26 23  1 20 30  1 23 26 33 16]\n",
      " [26 26 22  6  1 31 19 20 30  1]\n",
      " [19 16 16  1 29 16 30 26 29 31]\n",
      " [16 12 29 16 29  6  0 13 32 31]\n",
      " [27 32 31  3 30 31  1 17 26 29]]\n",
      "\n",
      "y\n",
      " [[19 16  1 30 26 25 25 16 31 30]\n",
      " [20 25 18  1 24 16 25  8  1  1]\n",
      " [16 30 16  1 14 32 29 20 26 32]\n",
      " [23  1 20 30  1 23 26 33 16  6]\n",
      " [26 22  6  1 31 19 20 30  1 23]\n",
      " [16 16  1 29 16 30 26 29 31  9]\n",
      " [12 29 16 29  6  0 13 32 31  1]\n",
      " [32 31  3 30 31  1 17 26 29 31]]\n"
     ]
    }
   ],
   "source": [
    "##test get_batches \n",
    "\n",
    "batches = get_batches(encoded, 8,100)\n",
    "x,y = next(batches)\n",
    "\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "if gpu:\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Not Training on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining the network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn. Module):\n",
    "    def __init__(self,tokens, n_hidden=256, n_layers=2,drop_prob = 0.5,lr = 0.001):\n",
    "        super().__init__()\n",
    "        ## Class variables\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        ## create class dictionaries \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n",
    "        \n",
    "        ## Create the layers \n",
    "        ## I am not using any embedding here \n",
    "        input_size = len(self.chars)\n",
    "        output_size = len(self.chars)\n",
    "        self.lstm = nn.LSTM(input_size, self.n_hidden, self.n_layers, \n",
    "                           dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.FC = nn.Linear(self.n_hidden, output_size)\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        r_out,hidden = self.lstm(x,hidden)\n",
    "        r_out = self.dropout(r_out)\n",
    "        \n",
    "        r_out = r_out.contiguous().reshape(-1,self.n_hidden)\n",
    "        \n",
    "        out = self.FC(r_out)\n",
    "        \n",
    "        return out,hidden\n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        ### initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weights = next(self.parameters()).data\n",
    "        \n",
    "        if gpu: ## Create Cuda tensors\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data,epochs =1,batch_size = 10,seq_len =10,lr = 0.001,clip = 5, val_frac= 0.1, print_freq = 10):\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    ##optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model_name = 'char_rnn.net'\n",
    "    \n",
    "    \n",
    "    ##Create train and Validation set \n",
    "    val_idx = int(len(data)*(1- val_frac))\n",
    "    train_data, valid_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    #### counter and vocab size\n",
    "    vocab_size = len(model.chars)\n",
    "    minimum_val_loss =np.inf\n",
    "    ##minimum_val_loss =3.1185\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        counter = 0\n",
    "        train\n",
    "        \n",
    "        h = model.init_hidden(batch_size)\n",
    "        for x,y in get_batches(train_data,batch_size,seq_len):\n",
    "            model.train()\n",
    "            counter +=1\n",
    "            x = one_hot(x,vocab_size)\n",
    "            inputs = torch.from_numpy(x)\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            labels = torch.from_numpy(y)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "            if gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            h = tuple([each.data for each in h]) ## this is already in cuda if GPU is available\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output,h = model(inputs,h)\n",
    "            labels = labels.type(torch.cuda.LongTensor)\n",
    "            loss = criterion(output,labels.view(batch_size*seq_len))\n",
    "            loss.backward()            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if counter% print_freq == 0:\n",
    "                model.eval()\n",
    "                val_losses =[]\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                for val_x,val_y in get_batches(valid_data,batch_size,seq_len):\n",
    "                    val_x= one_hot(val_x,vocab_size)\n",
    "                    inputs, labels = torch.from_numpy(val_x), torch.from_numpy(val_y)\n",
    "                    inputs = inputs.type(torch.FloatTensor)\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    inputs,labels = inputs.cuda(),labels.cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    output,val_h = model(inputs,val_h)\n",
    "                    labels = labels.type(torch.cuda.LongTensor)\n",
    "                    loss= criterion(output,labels.view(batch_size*seq_len))\n",
    "                    val_losses.append(loss.item())\n",
    "  \n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "                ## Save Model if validation loss goes down\n",
    "                if (np.mean(val_losses) < minimum_val_loss):\n",
    "                    minimum_val_loss = np.mean(val_losses)\n",
    "                    checkpoint = {'n_hidden' : model.n_hidden,\n",
    "                                  'n_layers' : model.n_layers,\n",
    "                                   'state_dict': model.state_dict(),\n",
    "                                   'tokens' :model.chars}\n",
    "                    print('Saving Model...')\n",
    "        \n",
    "                with open(model_name, 'wb') as f:\n",
    "                    torch.save(checkpoint,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(38, 512, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (FC): Linear(in_features=512, out_features=38, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##Hyperparameters\n",
    "\n",
    "n_hidden= 512\n",
    "n_layers=3\n",
    "drop_prob = 0.1\n",
    "lr = 0.001\n",
    "\n",
    "model = CharRNN(words,n_hidden,n_layers,drop_prob,lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/22... Step: 10... Loss: 3.0081... Val Loss: 3.0300\n",
      "Saving Model...\n",
      "Epoch: 1/22... Step: 20... Loss: 2.9948... Val Loss: 3.0066\n",
      "Saving Model...\n",
      "Epoch: 2/22... Step: 10... Loss: 2.9874... Val Loss: 2.9965\n",
      "Saving Model...\n",
      "Epoch: 2/22... Step: 20... Loss: 2.9881... Val Loss: 2.9948\n",
      "Saving Model...\n",
      "Epoch: 3/22... Step: 10... Loss: 2.9860... Val Loss: 2.9926\n",
      "Saving Model...\n",
      "Epoch: 3/22... Step: 20... Loss: 2.9853... Val Loss: 2.9920\n",
      "Saving Model...\n",
      "Epoch: 4/22... Step: 10... Loss: 2.9493... Val Loss: 2.9551\n",
      "Saving Model...\n",
      "Epoch: 4/22... Step: 20... Loss: 2.9540... Val Loss: 2.9584\n",
      "Epoch: 5/22... Step: 10... Loss: 2.7863... Val Loss: 2.7893\n",
      "Saving Model...\n",
      "Epoch: 5/22... Step: 20... Loss: 2.7270... Val Loss: 2.7274\n",
      "Saving Model...\n",
      "Epoch: 6/22... Step: 10... Loss: 2.6257... Val Loss: 2.6219\n",
      "Saving Model...\n",
      "Epoch: 6/22... Step: 20... Loss: 2.5582... Val Loss: 2.5491\n",
      "Saving Model...\n",
      "Epoch: 7/22... Step: 10... Loss: 2.4208... Val Loss: 2.4132\n",
      "Saving Model...\n",
      "Epoch: 7/22... Step: 20... Loss: 2.3527... Val Loss: 2.3485\n",
      "Saving Model...\n",
      "Epoch: 8/22... Step: 10... Loss: 2.2631... Val Loss: 2.2497\n",
      "Saving Model...\n",
      "Epoch: 8/22... Step: 20... Loss: 2.2264... Val Loss: 2.2098\n",
      "Saving Model...\n",
      "Epoch: 9/22... Step: 10... Loss: 2.1658... Val Loss: 2.1428\n",
      "Saving Model...\n",
      "Epoch: 9/22... Step: 20... Loss: 2.1541... Val Loss: 2.1275\n",
      "Saving Model...\n",
      "Epoch: 10/22... Step: 10... Loss: 2.1114... Val Loss: 2.0715\n",
      "Saving Model...\n",
      "Epoch: 10/22... Step: 20... Loss: 2.1004... Val Loss: 2.0590\n",
      "Saving Model...\n",
      "Epoch: 11/22... Step: 10... Loss: 2.0435... Val Loss: 1.9948\n",
      "Saving Model...\n",
      "Epoch: 11/22... Step: 20... Loss: 2.0264... Val Loss: 1.9788\n",
      "Saving Model...\n",
      "Epoch: 12/22... Step: 10... Loss: 2.0081... Val Loss: 1.9489\n",
      "Saving Model...\n",
      "Epoch: 12/22... Step: 20... Loss: 1.9755... Val Loss: 1.9193\n",
      "Saving Model...\n",
      "Epoch: 13/22... Step: 10... Loss: 1.9571... Val Loss: 1.8967\n",
      "Saving Model...\n",
      "Epoch: 13/22... Step: 20... Loss: 1.9420... Val Loss: 1.8816\n",
      "Saving Model...\n",
      "Epoch: 14/22... Step: 10... Loss: 1.9242... Val Loss: 1.8623\n",
      "Saving Model...\n",
      "Epoch: 14/22... Step: 20... Loss: 1.9110... Val Loss: 1.8492\n",
      "Saving Model...\n",
      "Epoch: 15/22... Step: 10... Loss: 1.8995... Val Loss: 1.8283\n",
      "Saving Model...\n",
      "Epoch: 15/22... Step: 20... Loss: 1.8807... Val Loss: 1.8174\n",
      "Saving Model...\n",
      "Epoch: 16/22... Step: 10... Loss: 1.8771... Val Loss: 1.8054\n",
      "Saving Model...\n",
      "Epoch: 16/22... Step: 20... Loss: 1.8535... Val Loss: 1.7883\n",
      "Saving Model...\n",
      "Epoch: 17/22... Step: 10... Loss: 1.8772... Val Loss: 1.8018\n",
      "Epoch: 17/22... Step: 20... Loss: 1.8435... Val Loss: 1.7720\n",
      "Saving Model...\n",
      "Epoch: 18/22... Step: 10... Loss: 1.8489... Val Loss: 1.7809\n",
      "Epoch: 18/22... Step: 20... Loss: 1.8354... Val Loss: 1.7626\n",
      "Saving Model...\n",
      "Epoch: 19/22... Step: 10... Loss: 1.8343... Val Loss: 1.7600\n",
      "Saving Model...\n",
      "Epoch: 19/22... Step: 20... Loss: 1.8167... Val Loss: 1.7479\n",
      "Saving Model...\n",
      "Epoch: 20/22... Step: 10... Loss: 1.8192... Val Loss: 1.7448\n",
      "Saving Model...\n",
      "Epoch: 20/22... Step: 20... Loss: 1.8109... Val Loss: 1.7415\n",
      "Saving Model...\n",
      "Epoch: 21/22... Step: 10... Loss: 1.8195... Val Loss: 1.7423\n",
      "Epoch: 21/22... Step: 20... Loss: 1.8112... Val Loss: 1.7373\n",
      "Saving Model...\n",
      "Epoch: 22/22... Step: 10... Loss: 1.8082... Val Loss: 1.7315\n",
      "Saving Model...\n",
      "Epoch: 22/22... Step: 20... Loss: 1.7952... Val Loss: 1.7258\n",
      "Saving Model...\n"
     ]
    }
   ],
   "source": [
    "data= encoded\n",
    "epochs = 22\n",
    "batch_size = 100\n",
    "seq_len = 30\n",
    "lr = 0.001\n",
    "clip = 5\n",
    "val_frac= 0.1\n",
    "##print_freq = 10\n",
    "\n",
    "##Train the model\n",
    "train(model,data,epochs,batch_size,seq_len,lr,clip, val_frac= 0.1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "with open('char_rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "model = CharRNN(checkpoint['tokens'],n_hidden =checkpoint['n_hidden'],n_layers=checkpoint['n_layers'])\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction using this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char,h = None, top_k = None):\n",
    "    ## char is the input charecter here\n",
    "    ##model.eval() (while call eval during priming)\n",
    "    vocab_size = len(model.chars)\n",
    "    x = np.array([[char2int[char]]]) ## multipli dimentional\n",
    "    x = one_hot(x,vocab_size)\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x.type(torch.FloatTensor)\n",
    "    if gpu:\n",
    "        x= x.cuda()\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    output, h = model(x,h)\n",
    "    p= F.softmax(output,dim =1).data\n",
    "    if gpu:\n",
    "        p = p.cpu()\n",
    "    if top_k ==None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch= top_ch.numpy().squeeze()\n",
    "    p = p.numpy().squeeze()\n",
    "    ch = np.random.choice(top_ch, p = p/p.sum())\n",
    "    character = int2char[ch]\n",
    "    return character,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Priming and generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, prime='The', top_k=None):\n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1)  ## Batch size is 1 \n",
    "    model.eval()\n",
    "    for ch in prime:\n",
    "        char,h = predict(model, ch,h , top_k = top_k)\n",
    "    chars.append(char) ## just add the last one after prime\n",
    "    \n",
    "    for i in range(size):\n",
    "        char,h = predict(model, chars[-1],h , top_k = top_k)\n",
    "        chars.append(char)\n",
    "        \n",
    "    return ''.join(chars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text =generate_text(model, size=200, prime='thee', top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thee all wors in my self this show,\n",
      "time's thou ast to me.\n",
      "\n",
      "fear should a palse, all his prowounged worst breath, and sturn to be tee,\n",
      "that sime as time's feeds, thou art so tell,\n",
      "thou art when, when i mak\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HAHA!! It was fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
